{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9d9e46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import base64\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e981efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# !pip install langchain langchain-community langchain-google-genai\n",
    "# !pip install pymongo motor\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pypdf2 pdfplumber PyMuPDF\n",
    "# !pip install faiss-cpu\n",
    "# !pip install rank-bm25\n",
    "# !pip install python-docx\n",
    "# !pip install google-generativeai\n",
    "# !pip install clip-by-openai torch torchvision\n",
    "# !pip install transformers\n",
    "#!pip install langchain-experimental\n",
    "#!pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd38f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS, MongoDBAtlasVectorSearch\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027a3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AgenticAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Additional imports\n",
    "import fitz  # PyMuPDF\n",
    "import google.generativeai as genai\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from docx import Document as DocxDocument\n",
    "import pymongo\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f38b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "PDF_PATH = \"llama2.pdf\"\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c311ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "# import sys\n",
    "\n",
    "# # Replace the placeholder data with your Atlas connection string. Be sure it includes\n",
    "# # a valid username and password! Note that in a production environment,\n",
    "# # you should not store your password in plain-text here.\n",
    "\n",
    "# try:\n",
    "#   client = pymongo.MongoClient(MONGODB_URI)\n",
    "  \n",
    "# # return a friendly error if a URI error is thrown \n",
    "# except pymongo.errors.ConfigurationError:\n",
    "#   print(\"An Invalid URI host error was received. Is your Atlas host name correct in your connection string?\")\n",
    "#   sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965cebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 77 pages...\n",
      "Extracted 143 documents\n"
     ]
    }
   ],
   "source": [
    "# Extract PDF Data\n",
    "def extract_pdf_content(pdf_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    \n",
    "    print(f\"Processing {len(pdf_doc)} pages...\")\n",
    "    \n",
    "    for page_num in range(len(pdf_doc)):\n",
    "        page = pdf_doc[page_num]\n",
    "        \n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={'page': page_num + 1, 'type': 'text'}\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Extract images as base64\n",
    "        images = page.get_images()\n",
    "        for img_idx, img in enumerate(images):\n",
    "            try:\n",
    "                xref = img[0]\n",
    "                pix = fitz.Pixmap(pdf_doc, xref)\n",
    "                if pix.n - pix.alpha < 4:\n",
    "                    img_data = pix.tobytes(\"png\")\n",
    "                    img_b64 = base64.b64encode(img_data).decode()\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=f\"Image on page {page_num + 1}\",\n",
    "                        metadata={\n",
    "                            'page': page_num + 1, \n",
    "                            'type': 'image',\n",
    "                            'image_data': img_b64\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                pix = None\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Extract tables\n",
    "        tables = page.find_tables()\n",
    "        for table_idx, table in enumerate(tables):\n",
    "            try:\n",
    "                table_data = table.extract()\n",
    "                table_text = '\\n'.join([' | '.join([str(cell) for cell in row if cell]) for row in table_data if row])\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=table_text,\n",
    "                    metadata={'page': page_num + 1, 'type': 'table'}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    return documents\n",
    "\n",
    "# Extract documents\n",
    "documents = extract_pdf_content(PDF_PATH)\n",
    "print(f\"Extracted {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d2df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varig\\AppData\\Local\\Temp\\ipykernel_27976\\2113020550.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  base_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings for semantic chunking\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Use SemanticChunker for better semantic chunking\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=base_embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98fd712a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 327 chunks using SemanticChunker\n"
     ]
    }
   ],
   "source": [
    "# Apply semantic chunking\n",
    "chunked_docs = []\n",
    "for doc in documents:\n",
    "    if doc.metadata['type'] == 'text':\n",
    "        # Use semantic chunking for text\n",
    "        chunks = semantic_splitter.split_text(doc.page_content)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={**doc.metadata, 'chunk_id': i}\n",
    "            )\n",
    "            chunked_docs.append(chunked_doc)\n",
    "    else:\n",
    "        # Keep images and tables as is\n",
    "        chunked_docs.append(doc)\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks using SemanticChunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e5ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git\n",
    "import clip\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2296194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c713b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#Create Multimodal Embeddings\n",
    "# Load CLIP model for multimodal embeddings\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Text embeddings\n",
    "text_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "454c2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating multimodal embeddings...\n",
      "Multimodal embeddings created in 9.58 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_multimodal_embeddings(docs):\n",
    "    embeddings_list = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        if doc.metadata['type'] == 'text':\n",
    "            # Text embedding\n",
    "            embedding = text_embeddings.embed_query(doc.page_content)\n",
    "            embeddings_list.append(embedding)\n",
    "            \n",
    "        elif doc.metadata['type'] == 'image':\n",
    "            # Image embedding using CLIP\n",
    "            try:\n",
    "                img_data = base64.b64decode(doc.metadata['image_data'])\n",
    "                image = Image.open(io.BytesIO(img_data))\n",
    "                image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    image_features = clip_model.encode_image(image_input)\n",
    "                    # Normalize and convert to list\n",
    "                    embedding = image_features.cpu().numpy().flatten().tolist()\n",
    "                    # Pad/truncate to match text embedding dimension (384)\n",
    "                    if len(embedding) > 384:\n",
    "                        embedding = embedding[:384]\n",
    "                    else:\n",
    "                        embedding.extend([0.0] * (384 - len(embedding)))\n",
    "                    \n",
    "                embeddings_list.append(embedding)\n",
    "            except:\n",
    "                # Fallback to text embedding of description\n",
    "                embedding = text_embeddings.embed_query(doc.page_content)\n",
    "                embeddings_list.append(embedding)\n",
    "                \n",
    "        elif doc.metadata['type'] == 'table':\n",
    "            # Table embedding (treat as text)\n",
    "            embedding = text_embeddings.embed_query(doc.page_content)\n",
    "            embeddings_list.append(embedding)\n",
    "    \n",
    "    return embeddings_list\n",
    "\n",
    "# Create multimodal embeddings\n",
    "print(\"Creating multimodal embeddings...\")\n",
    "start_time = time.time()\n",
    "embedded_chunks = create_multimodal_embeddings(chunked_docs)\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"Multimodal embeddings created in {embedding_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ecceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MongoDB connection successful!\n",
      "Storing multimodal embeddings in MongoDB...\n",
      "MongoDB setup complete!\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import quote_plus\n",
    "#from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Encode credentials\n",
    "username = quote_plus(os.getenv(\"MONGODB_USERNAME\"))\n",
    "password = quote_plus(os.getenv(\"MONGODB_PASSWORD\"))\n",
    "\n",
    "# Replace with your actual cluster host (e.g., cluster0.abcde.mongodb.net)\n",
    "MONGODB_URI = (\n",
    "    f\"mongodb+srv://{username}:{password}@cluster0.gk1jj9u.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    ")\n",
    "\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[\"rag_database_new\"]\n",
    "collection = db[\"document_embeddings_manual\"]\n",
    "\n",
    "# Just check connection\n",
    "print(\"✅ MongoDB connection successful!\")\n",
    "\n",
    "\n",
    "\n",
    "# # MongoDB Atlas Vector Search setup\n",
    "# vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "#     documents=chunked_docs,\n",
    "#     embedding=text_embeddings,\n",
    "#     connection_string=MONGODB_URI,\n",
    "#     database_name=\"rag_database\",\n",
    "#     collection_name=\"document_embeddings\",\n",
    "#     index_name=\"vector_index\"\n",
    "# )\n",
    "\n",
    "# print(\"MongoDB Atlas Vector Store created!\")\n",
    "\n",
    "\n",
    "\n",
    "# Store documents with multimodal embeddings\n",
    "print(\"Storing multimodal embeddings in MongoDB...\")\n",
    "for i, (doc, embedding) in enumerate(zip(chunked_docs, embedded_chunks)):\n",
    "    document = {\n",
    "        \"_id\": f\"doc_{i}\",\n",
    "        \"content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"embedding\": embedding,\n",
    "        \"content_type\": doc.metadata['type']\n",
    "    }\n",
    "    collection.replace_one({\"_id\": f\"doc_{i}\"}, document, upsert=True)\n",
    "\n",
    "# Create MongoDB Atlas Vector Search Index (run this in MongoDB Atlas UI)\n",
    "index_definition = {\n",
    "    \"fields\": [{\n",
    "        \"type\": \"vector\",\n",
    "        \"path\": \"embedding\",\n",
    "        \"numDimensions\": 384,\n",
    "        \"similarity\": \"cosine\"\n",
    "    }]\n",
    "}\n",
    "print(\"MongoDB setup complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f21bb9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Flat index...\n",
      "Creating HNSW index...\n",
      "Creating IVF index...\n",
      "All indexes created!\n"
     ]
    }
   ],
   "source": [
    "# Create Multiple Index Types\n",
    "# Flat Index (FAISS)\n",
    "print(\"Creating Flat index...\")\n",
    "flat_index = faiss.IndexFlatIP(384)  # Inner product for cosine similarity\n",
    "flat_index.add(np.array(embedded_chunks).astype('float32'))\n",
    "\n",
    "# HNSW Index\n",
    "print(\"Creating HNSW index...\")\n",
    "hnsw_index = faiss.IndexHNSWFlat(384, 32)\n",
    "hnsw_index.add(np.array(embedded_chunks).astype('float32'))\n",
    "\n",
    "# IVF Index\n",
    "print(\"Creating IVF index...\")\n",
    "quantizer = faiss.IndexFlatIP(384)\n",
    "ivf_index = faiss.IndexIVFFlat(quantizer, 384, min(100, len(embedded_chunks)//10))\n",
    "ivf_index.train(np.array(embedded_chunks).astype('float32'))\n",
    "ivf_index.add(np.array(embedded_chunks).astype('float32'))\n",
    "\n",
    "print(\"All indexes created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b7c895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Retrieval Pipeline\n",
    "def retrieve_documents(query: str, index_type: str = \"flat\", k: int = 5):\n",
    "    # Create multimodal query embedding\n",
    "    query_embedding = text_embeddings.embed_query(query)\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    \n",
    "    # Retrieve based on index type\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if index_type == \"flat\":\n",
    "        scores, indices = flat_index.search(query_vector, k)\n",
    "    elif index_type == \"hnsw\":\n",
    "        scores, indices = hnsw_index.search(query_vector, k)\n",
    "    elif index_type == \"ivf\":\n",
    "        scores, indices = ivf_index.search(query_vector, k)\n",
    "    \n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    # Get retrieved documents\n",
    "    retrieved_docs = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        if idx < len(chunked_docs):\n",
    "            retrieved_docs.append({\n",
    "                'document': chunked_docs[idx],\n",
    "                'score': float(score),\n",
    "                'content_type': chunked_docs[idx].metadata['type']\n",
    "            })\n",
    "    \n",
    "    return retrieved_docs, retrieval_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3359bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAT Index: 0.0010 seconds\n",
      "HNSW Index: 0.0000 seconds\n",
      "IVF Index: 0.0000 seconds\n",
      "Fastest index: HNSW\n"
     ]
    }
   ],
   "source": [
    "# Test Retrieval Speed\n",
    "test_query = \"explain about llama2\"\n",
    "\n",
    "# Test all index types\n",
    "index_types = [\"flat\", \"hnsw\", \"ivf\"]\n",
    "speed_results = {}\n",
    "\n",
    "for idx_type in index_types:\n",
    "    docs, ret_time = retrieve_documents(test_query, idx_type, k=5)\n",
    "    speed_results[idx_type] = ret_time\n",
    "    print(f\"{idx_type.upper()} Index: {ret_time:.4f} seconds\")\n",
    "\n",
    "# Find fastest\n",
    "fastest_index = min(speed_results, key=speed_results.get)\n",
    "print(f\"Fastest index: {fastest_index.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ef802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores:\n",
      "Document 1: 0.5991 (Type: text, Page: 2)\n",
      "Document 2: 0.5936 (Type: text, Page: 2)\n",
      "Document 3: 0.5429 (Type: table, Page: 77)\n",
      "Document 4: 0.5348 (Type: table, Page: 77)\n",
      "Document 5: 0.5249 (Type: text, Page: 77)\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy_scores(query: str, retrieved_docs: List, k: int = 5):\n",
    "    query_embedding = text_embeddings.embed_query(query)\n",
    "    \n",
    "    accuracy_scores = []\n",
    "    for item in retrieved_docs[:k]:\n",
    "        if item['content_type'] == 'text':\n",
    "            doc_embedding = text_embeddings.embed_query(item['document'].page_content)\n",
    "        else:\n",
    "            # For images/tables, use the pre-computed embedding\n",
    "            doc_idx = chunked_docs.index(item['document'])\n",
    "            doc_embedding = embedded_chunks[doc_idx]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        cosine_sim = np.dot(query_embedding, doc_embedding) / (\n",
    "            np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "        )\n",
    "        accuracy_scores.append({\n",
    "            'score': cosine_sim,\n",
    "            'content_type': item['content_type'],\n",
    "            'page': item['document'].metadata.get('page', 'N/A')\n",
    "        })\n",
    "    \n",
    "    return accuracy_scores\n",
    "\n",
    "# Test accuracy\n",
    "retrieved_docs, _ = retrieve_documents(test_query, fastest_index, k=5)\n",
    "accuracy_scores = calculate_accuracy_scores(test_query, retrieved_docs)\n",
    "\n",
    "print(\"Accuracy Scores:\")\n",
    "for i, score_info in enumerate(accuracy_scores):\n",
    "    print(f\"Document {i+1}: {score_info['score']:.4f} (Type: {score_info['content_type']}, Page: {score_info['page']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ab80646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked documents ready!\n"
     ]
    }
   ],
   "source": [
    "# Reranking with BM25\n",
    "# Prepare BM25\n",
    "tokenized_docs = [doc.page_content.split() for doc in chunked_docs]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def rerank_with_bm25(query: str, retrieved_docs: List, top_k: int = 3):\n",
    "    # Get BM25 scores\n",
    "    tokenized_query = query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Rerank retrieved documents\n",
    "    reranked = []\n",
    "    for item in retrieved_docs:\n",
    "        doc_idx = chunked_docs.index(item['document'])\n",
    "        combined_score = item['score'] * 0.7 + bm25_scores[doc_idx] * 0.3\n",
    "        reranked.append({\n",
    "            'document': item['document'],\n",
    "            'vector_score': item['score'],\n",
    "            'bm25_score': bm25_scores[doc_idx],\n",
    "            'combined_score': combined_score\n",
    "        })\n",
    "    \n",
    "    # Sort by combined score\n",
    "    reranked.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "    return reranked[:top_k]\n",
    "\n",
    "# Apply reranking\n",
    "reranked_docs = rerank_with_bm25(test_query, retrieved_docs, top_k=3)\n",
    "print(\"Reranked documents ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7437732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'document': Document(metadata={'page': 77, 'type': 'text', 'chunk_id': 2}, page_content='The fine-tuning data includes publicly available instruction datasets, as\\nwell as over one million new human-annotated examples. Neither the pretraining\\nnor the fine-tuning datasets include Meta user data. Data Freshness\\nThe pretraining data has a cutoff of September 2022, but some tuning data is\\nmore recent, up to July 2023. Evaluation Results\\nSee evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4). Ethical Considerations and Limitations (Section 5.2)\\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications of Llama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for Llama 2. 77\\n'),\n",
       "  'vector_score': 0.9501253366470337,\n",
       "  'bm25_score': np.float64(0.0),\n",
       "  'combined_score': np.float64(0.6650877356529236)},\n",
       " {'document': Document(metadata={'page': 77, 'type': 'table'}, page_content='Intended Use Cases | Llama 2 is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses | Use in any manner that violates applicable laws or regulations (including trade\\ncompliance laws). Use in languages other than English. Use in any other way\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nLlama 2.'),\n",
       "  'vector_score': 0.9303056001663208,\n",
       "  'bm25_score': np.float64(0.0),\n",
       "  'combined_score': np.float64(0.6512139201164245)},\n",
       " {'document': Document(metadata={'page': 77, 'type': 'table'}, page_content='Overview | Llama 2 was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as\\nwell as over one million new human-annotated examples. Neither the pretraining\\nnor the fine-tuning datasets include Meta user data.\\nData Freshness | The pretraining data has a cutoff of September 2022, but some tuning data is\\nmore recent, up to July 2023.'),\n",
       "  'vector_score': 0.9141601324081421,\n",
       "  'bm25_score': np.float64(0.0),\n",
       "  'combined_score': np.float64(0.6399120926856994)}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af04e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prompt Template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a comprehensive answer based on the context above. If the context doesn't contain enough information, say so clearly.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecfb8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall protobuf -y\n",
    "# !pip install protobuf==4.23.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98315e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "Llama 2 is a large language model (LLM) pretrained on 2 trillion tokens of publicly available data (cutoff September 2022).  Its fine-tuning involved publicly available instruction datasets and over one million new human-annotated examples.  Importantly, neither the pretraining nor fine-tuning data included Meta user data.  Some of the fine-tuning data is more recent, with a cutoff of July 2023.\n",
      "\n",
      "Llama 2 is intended for commercial and research use in English.  The tuned models are designed for assistant-like chat applications, while the pretrained models can be adapted for various natural language generation tasks.  Its use is restricted; it cannot be used in ways that violate applicable laws or regulations, in languages other than English, or in any manner prohibited by Llama 2's Acceptable Use Policy and Licensing Agreement.\n",
      "\n",
      "While testing has been conducted in English, it hasn't covered all scenarios.  Therefore, Llama 2 may produce inaccurate or objectionable responses, and developers are advised to conduct their own safety testing and tuning before deploying any applications.  A Responsible Use Guide is available at https://ai.meta.com/llama/responsible-user-guide.\n"
     ]
    }
   ],
   "source": [
    "# Generate Output with LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1)\n",
    "\n",
    "def generate_answer(query: str, retrieved_docs: List):\n",
    "    # Prepare context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {doc['document'].page_content}\" \n",
    "        for i, doc in enumerate(retrieved_docs)\n",
    "    ])\n",
    "    \n",
    "    # Generate response\n",
    "    formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(test_query, reranked_docs)\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a24a7d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to rag_output.docx\n",
      "RAG Pipeline Complete!\n"
     ]
    }
   ],
   "source": [
    "# Create DOCX Output\n",
    "def create_docx_output(query: str, answer: str, retrieved_docs: List, filename: str = \"rag_output.docx\"):\n",
    "    doc = DocxDocument()\n",
    "    \n",
    "    # Title\n",
    "    doc.add_heading('RAG Pipeline Output', 0)\n",
    "    \n",
    "    # Query\n",
    "    doc.add_heading('Query:', level=1)\n",
    "    doc.add_paragraph(query)\n",
    "    \n",
    "    # Answer\n",
    "    doc.add_heading('Generated Answer:', level=1)\n",
    "    doc.add_paragraph(answer)\n",
    "    \n",
    "    # Retrieved Documents\n",
    "    doc.add_heading('Retrieved Documents:', level=1)\n",
    "    for i, item in enumerate(retrieved_docs):\n",
    "        doc.add_heading(f'Document {i+1}:', level=2)\n",
    "        doc.add_paragraph(f\"Score: {item.get('combined_score', item.get('score', 0)):.4f}\")\n",
    "        doc.add_paragraph(f\"Content: {item['document'].page_content[:500]}...\")\n",
    "        doc.add_paragraph(f\"Metadata: {item['document'].metadata}\")\n",
    "    \n",
    "    doc.save(filename)\n",
    "    print(f\"Output saved to {filename}\")\n",
    "\n",
    "# Create DOCX output\n",
    "create_docx_output(test_query, answer, reranked_docs)\n",
    "\n",
    "print(\"RAG Pipeline Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b6f7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
